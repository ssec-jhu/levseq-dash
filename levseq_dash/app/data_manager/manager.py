import os
from collections import defaultdict
from pathlib import Path

import pandas as pd

from levseq_dash.app import global_strings as gs
from levseq_dash.app.config import settings
from levseq_dash.app.config.settings import StorageMode
from levseq_dash.app.data_manager.experiment import Experiment, MutagenesisMethod
from levseq_dash.app.utils.utils import log_with_context

# from levseq_dash.app.wsexec import Query


class DataManager:
    def __init__(self):
        """Initialize the database"""
        if settings.is_db_mode():
            self.use_db_web_service = StorageMode.db.value
            # TODO: database connection happens on instance load
            # ideally defined in the config file
            # connect_to_db( host, port, username, password)
        elif settings.is_disk_mode():
            self.use_db_web_service = StorageMode.disk.value

            # Set up the data path
            self._setup_data_path()

            # read the assay file and set up the assay list
            self._load_assay_list()

            # create a default empty dictionary of experiments
            self.experiments_dict = defaultdict(Experiment)

            # use this flag for debugging multiple files.
            # This will load all csv files in test/data
            # function below will load test data files and their geometry from the
            # data folder and fill self.experiments_dict
            self._load_test_experiment_data()

        else:
            raise Exception(gs.error_data_mode)

    # -----------------------
    #       ADD DATA
    # -----------------------

    def add_experiment_from_ui(
        self,
        user_id,
        experiment_name,
        experiment_date,
        substrate,
        product,
        assay,
        mutagenesis_method: MutagenesisMethod,  # epPCR or SSM
        experiment_content_base64_string,
        geometry_content_base64_string,
        # parent_sequence=None,  # processed
    ) -> int:
        """
        Returns
        -------
        An experiment ID is automatically generated by the database and returned for future reference.
        """
        n = -1
        if self.use_db_web_service == StorageMode.db.value:
            pass

        elif self.use_db_web_service == StorageMode.disk.value:
            exp = Experiment(
                experiment_csv_data_base64_string=experiment_content_base64_string,
                experiment_name=experiment_name,
                experiment_date=experiment_date,
                substrate=substrate,
                product=product,
                assay=assay,
                mutagenesis_method=mutagenesis_method,
                geometry_file_path=None,
                geometry_base64_string=geometry_content_base64_string,
            )
            n = self._add_experiment(exp)
        else:
            raise Exception(gs.error_data_mode)
        return n

    # ---------------------------
    #    Delete
    # ---------------------------
    def delete_experiment(self, experiment_id: int) -> bool:
        """
            Delete the experiment associated with this id

        Returns
        -------
            bool True if deleted successfully

        """
        success = False
        if self.use_db_web_service == StorageMode.db.value:
            pass
        elif self.use_db_web_service == StorageMode.disk.value:
            del self.experiments_dict[experiment_id]
            success = True
        else:
            raise Exception(gs.error_data_mode)

        return success

    # ---------------------------
    #    DATA RETRIEVAL: ALL
    # ---------------------------
    def get_all_lab_experiments_with_meta_data(self):
        """
        Returns
        -------
        | user_id | user_name| experiment_id | experiment_name | upload_time_stamp | experiment_date |
        substrate | product | assay | mutagenesis_method
        """
        data_list_of_dict = []
        if self.use_db_web_service == StorageMode.db.value:
            # TODO:
            #  get all lab experiments ids + get metadata from each experiment
            #  OR: this can be bundled into 1 function
            #  put data together
            pass
        elif self.use_db_web_service == StorageMode.disk.value:
            # get the metadata for all the experiments
            for key, exp in self.experiments_dict.items():
                exp_data = exp.exp_meta_data_to_dict()
                exp_data["experiment_id"] = key
                data_list_of_dict.append(exp_data)

            # TODO: is below faster? this has dictionary unpacking? does it have overhead?
            # data_list = [
            #     {"experiment_id": key, **exp.exp_meta_data_to_dict()}
            #     for key, exp in self.experiments_dict.items()
            # ]
        else:
            raise Exception(gs.error_data_mode)
        return data_list_of_dict

    def get_all_lab_sequences(self):
        """
        Returns
        -------
        | user | experiment_id | parent_sequence
        """
        seq_data = {}
        if self.use_db_web_service == StorageMode.db.value:
            # TODO:
            # get all lab sequences
            pass
        elif self.use_db_web_service == StorageMode.disk.value:
            # get the metadata for all the experiments
            # this make a list of dictionaries
            # for key, exp in self.experiments_dict.items():
            #     seq_data = {"experiment_id": key, "parent_sequence": exp.parent_sequence}
            #     data_list_of_dict.append(seq_data)

            # dictionary of "experiment id" and "experiment sequence" pairs
            # the key of the dictionary is the experiment ID
            for key, exp in self.experiments_dict.items():
                seq_data.update({key: exp.parent_sequence})
        else:
            raise Exception(gs.error_data_mode)
        return seq_data

    # ---------------------------
    #    DATA RETRIEVAL: PER EXPERIMENT
    # ---------------------------
    def get_experiment(self, experiment_id: int) -> Experiment | None:
        # helper function
        exp = None
        if self.use_db_web_service == StorageMode.db.value:
            # TODO:
            #  get_experiment_meta_data
            #  get_experiment_core_data
            #  get_experiment_parent_sequence
            #  get_experiment_geometry_file
            #  return Experiment
            pass
        elif self.use_db_web_service == StorageMode.disk.value:
            # use get so for a non-existent experiment id it
            # won't throw an exception
            exp = self.experiments_dict.get(experiment_id)
        else:
            raise Exception(gs.error_data_mode)
        return exp

    # ---------------------------
    #    DATA RETRIEVAL: MISC
    # ---------------------------
    def get_assays(self):
        """
        Returns list of assays
        -------
        | assay |
        """
        assay_list = []
        if self.use_db_web_service == StorageMode.db.value:
            pass

        elif self.use_db_web_service == StorageMode.disk.value:
            assay_list = self.assay_list
        else:
            raise Exception(gs.error_data_mode)
        return assay_list

    # ----------------------------
    #    PRIVATE METHODS
    # ---------------------------
    def _setup_data_path(self):
        """ """
        if settings.is_local_instance_mode():
            # local-instance mode: Use DATA_PATH if set, otherwise fall back to local dev path
            data_path_str = os.environ.get("DATA_PATH", settings.get_local_instance_mode_data_path())
            if data_path_str:
                data_path = Path(data_path_str)
                if data_path.is_absolute():
                    self.data_path = data_path.resolve()
                else:
                    # For relative paths, resolve from the app directory
                    self.data_path = (Path("app") / data_path).resolve()
            else:
                raise ValueError(
                    "local-instance MODE ERROR: No storage path configured!\n"
                    "Options:\n"
                    "1. Set DATA_PATH environment variable: "
                    " docker run -e DATA_PATH=/data -v /host/path:/data  <image-name>\n"
                    "2. Set local-data-path in config.yaml\n"
                )
        else:
            # Playground mode: Use DATA_PATH if provided, otherwise default
            self.data_path = Path(os.environ.get("DATA_PATH", "data/DEDB")).resolve()

        log_with_context(
            f"[LOG] Using path: {self.data_path})",
            log_flag=settings.is_data_manager_logging_enabled(),
        )

        # Sanity checks on the directory
        if not self.data_path.exists():
            raise FileNotFoundError(f"Data directory not found at {self.data_path}\n")

        # Check if the path is writable
        if settings.is_data_modification_enabled() and not os.access(self.data_path, os.W_OK):
            raise PermissionError(f"No write permission to storage: {self.data_path}\n")

    def _load_assay_list(self):
        if settings.assay_file_path.exists():
            assays = pd.read_csv(settings.assay_file_path, encoding="utf-8", usecols=["Technique"])
            self.assay_list = assays["Technique"].tolist()
            log_with_context(
                f"[LOG] Read assay file at: {settings.assay_file_path} with size {len(self.assay_list)}",
                log_flag=settings.is_data_manager_logging_enabled(),
            )

    def _load_test_experiment_data(self):
        """
        This method is only used for loading from disk for test purposes.
        It is not to be used outside of this context.
        It assumes the file structure in data_directory is set up as below:
        Project Directory Structure and Metadata Mapping

        /data/                        data_directory path
        ├── meta_data.csv             Metadata file describing experiments and structures
        ├── experiments/              Folder containing experiment result tables
        │   ├── experiment_0.csv      Data for experiment_0
        │   └── experiment_1.csv      Data for experiment_1
        │   └── ...
        └── structures/               Folder containing molecular geometry files (CIF format)
            ├── geometry_0.cif        3D structure associated with experiment_0
            └── geometry_1.cif        3D structure associated with experiment_1
        └── ...

        ----------------------------
        meta_data.csv Description
        ----------------------------

        This file provides a mapping between experiment metadata, result files, and structural data.

        Columns:
        ┌──────────────────────┬──────────────────────────────────────────────────────────────┐
        │ Column Name          │ Description                                                  │
        ├──────────────────────┼──────────────────────────────────────────────────────────────┤
        │ experiment_id        │ matches a file in /data/experiments/, e.g.,                  │
        │                      │ "experiment_0" → experiments/experiment_0.csv                │
        │ experiment_name      │ name of the experiment                                       │
        │ experiment_date      │ date the experiment was performed                            │
        │ substrate_smiles     │ SMILES string of the substrate                               │
        │ product_smiles       │ SMILES string of the product                                 │
        │ assay_technique      │ technique used (e.g., fluorescence, OD600)                   │
        │ cif_filename         │ matches a file in /data/structures/, e.g.,                   │
        │                      │ "geometry_0.cif" → structures/geometry_0.cif                 │
        └──────────────────────┴──────────────────────────────────────────────────────────────┘

        Example Row:
        experiment_id: experiment_0
        experiment_name: Substrate Optimization A
        experiment_date: 2025-05-20
        substrate_smiles: CC(=O)OC1=CC=CC=C1C(=O)O
        product_smiles: C1=CC=CC=C1COOH
        assay_technique: fluorescence
        cif_filename: geometry_0.cif
        """
        if self.use_db_web_service == StorageMode.db.value:
            raise Exception(gs.error_wrong_mode)

        data_directory = self.data_path
        experiment_dir = data_directory / "experiments"
        structures_dir = data_directory / "structures"
        meta_data_file = data_directory / "meta_data.csv"

        # validate required directories
        for directory in [data_directory, experiment_dir, structures_dir]:
            if not directory.is_dir():
                raise FileNotFoundError(f"Required directory does not exist: {directory}")

        log_with_context(
            f"[LOG] Found all data at: {data_directory}... reading experiments...",
            log_flag=settings.is_data_manager_logging_enabled(),
        )

        # read the file and iterate through metadata rows
        metadata_df = pd.read_csv(meta_data_file)
        for idx, row in metadata_df.iterrows():
            experiment_id = row["experiment_id"]
            experiment_name = row["experiment_name"]
            experiment_date = row["experiment_date"]
            sub_string = str(row["substrate_smiles"])
            experiment_substrate = (
                sub_string if sub_string and sub_string.lower() not in ["nan", "none", "null"] else ""
            )
            experiment_product = row["product_smiles"]
            assay = row["assay_technique"]
            exp_file_path = experiment_dir / f"{experiment_id}.csv"
            geometry_file_path = structures_dir / f"{row['cif_filename']}"

            # .get(..., '') defaults to an empty string ('') if the column is missing
            # but if it's empty it will return Nan
            mutagenesis_raw = str(row.get("mutagenesis_method", ""))
            if mutagenesis_raw.lower() == "ssm":
                mutagenesis_method = MutagenesisMethod.SSM
            else:
                mutagenesis_method = MutagenesisMethod.epPCR

            # Create the Experiment object
            try:
                exp = Experiment(
                    experiment_data_file_path=exp_file_path,
                    experiment_name=experiment_name,
                    experiment_date=experiment_date,
                    substrate=experiment_substrate,
                    product=experiment_product,
                    assay=assay,
                    mutagenesis_method=mutagenesis_method,
                    geometry_file_path=geometry_file_path,
                )

                # Add the experiment to the system
                self._add_experiment(exp)
            except Exception as e:
                log_with_context(
                    f"[LOG] Exception {e} thrown with experiment {exp_file_path}. Not adding it to the list. ",
                    log_flag=settings.is_data_manager_logging_enabled(),
                )

        log_with_context(
            f"[LOG] Done reading and adding all the experiments.", log_flag=settings.is_data_manager_logging_enabled()
        )

    def _add_experiment(self, exp: Experiment):
        if self.use_db_web_service == StorageMode.db.value:
            raise Exception(gs.error_wrong_mode)

        n = len(self.experiments_dict.items())
        self.experiments_dict[n] = exp
        return n


# Python will only run module-level code once per process, no matter how often Dash reloads pages or triggers callbacks
# this will ensure Dash doesn't recreate the instance every time the page reloads or a callback is triggered
singleton_data_mgr_instance = DataManager()
